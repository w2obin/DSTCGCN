PEMS04
Trainset:	x-(10181, 12, 307, 3)	y-(10181, 12, 307, 1)
Valset:  	x-(3394, 12, 307, 3)  	y-(3394, 12, 307, 1)
Testset:	x-(3394, 12, 307, 3)	y-(3394, 12, 307, 1)

--------- DSTCGCN ---------
{
    "num_nodes": 307,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0005,
    "milestones": [
        15,
        30,
        50
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 5,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 307,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
DSTCGCN                                  [16, 12, 307, 1]          297,910
├─Linear: 1-1                            [16, 12, 307, 24]         96
├─Embedding: 1-2                         [16, 12, 307, 24]         6,912
├─Embedding: 1-3                         [16, 12, 307, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 307, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 307, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 307, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 307, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 307, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 307, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 307, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 307, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 307, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 307, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 307, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 307, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 307, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 307, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 307, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 307, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 307, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 307, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 307, 152]        304
├─ModuleList: 1-6                        --                        --
│    └─FFTSelector: 2-7                  [12, 3]                   --
│    │    └─Linear: 3-37                 [16, 12, 256]             11,946,240
│    │    └─Linear: 3-38                 [16, 12, 256]             11,946,240
├─ModuleList: 1-7                        --                        --
│    └─CGCN: 2-8                         [16, 12, 307, 152]        618
│    │    └─LayerNorm: 3-39              [307, 10]                 20
│    │    └─Dropout: 3-40                [307, 10]                 --
│    │    └─GCN: 3-41                    [16, 921, 152]            463,904
│    │    └─LayerNorm: 3-42              [307, 10]                 (recursive)
│    │    └─Dropout: 3-43                [307, 10]                 --
│    │    └─GCN: 3-44                    [16, 921, 152]            (recursive)
│    │    └─LayerNorm: 3-45              [307, 10]                 (recursive)
│    │    └─Dropout: 3-46                [307, 10]                 --
│    │    └─GCN: 3-47                    [16, 921, 152]            (recursive)
│    │    └─LayerNorm: 3-48              [307, 10]                 (recursive)
│    │    └─Dropout: 3-49                [307, 10]                 --
│    │    └─GCN: 3-50                    [16, 921, 152]            (recursive)
│    │    └─LayerNorm: 3-51              [307, 10]                 (recursive)
│    │    └─Dropout: 3-52                [307, 10]                 --
│    │    └─GCN: 3-53                    [16, 921, 152]            (recursive)
│    │    └─LayerNorm: 3-54              [307, 10]                 (recursive)
│    │    └─Dropout: 3-55                [307, 10]                 --
│    │    └─GCN: 3-56                    [16, 921, 152]            (recursive)
│    │    └─LayerNorm: 3-57              [307, 10]                 (recursive)
│    │    └─Dropout: 3-58                [307, 10]                 --
│    │    └─GCN: 3-59                    [16, 921, 152]            (recursive)
│    │    └─LayerNorm: 3-60              [307, 10]                 (recursive)
│    │    └─Dropout: 3-61                [307, 10]                 --
│    │    └─GCN: 3-62                    [16, 921, 152]            (recursive)
│    │    └─LayerNorm: 3-63              [307, 10]                 (recursive)
│    │    └─Dropout: 3-64                [307, 10]                 --
│    │    └─GCN: 3-65                    [16, 921, 152]            (recursive)
│    │    └─LayerNorm: 3-66              [307, 10]                 (recursive)
│    │    └─Dropout: 3-67                [307, 10]                 --
│    │    └─GCN: 3-68                    [16, 921, 152]            (recursive)
│    │    └─LayerNorm: 3-69              [307, 10]                 (recursive)
│    │    └─Dropout: 3-70                [307, 10]                 --
│    │    └─GCN: 3-71                    [16, 921, 152]            (recursive)
│    │    └─LayerNorm: 3-72              [307, 10]                 (recursive)
│    │    └─Dropout: 3-73                [307, 10]                 --
│    │    └─GCN: 3-74                    [16, 921, 152]            (recursive)
│    │    └─LayerNorm: 3-75              [16, 12, 307, 152]        304
├─LayerNorm: 1-8                         [16, 12, 307, 152]        304
├─Linear: 1-9                            [16, 307, 12]             21,900
==========================================================================================
Total params: 25,715,800
Trainable params: 25,715,800
Non-trainable params: 0
Total mult-adds (M): 399.39
==========================================================================================
Input size (MB): 0.71
Forward/backward pass size (MB): 4128.58
Params size (MB): 99.81
Estimated Total Size (MB): 4229.10
==========================================================================================

Loss: HuberLoss

2024-09-13 06:02:22.095567 Epoch 1  	Train Loss = 36.95469 Val Loss = 27.59656
2024-09-13 06:04:44.746914 Epoch 2  	Train Loss = 24.68747 Val Loss = 24.70962
2024-09-13 06:07:07.944211 Epoch 3  	Train Loss = 22.86119 Val Loss = 21.56667
2024-09-13 06:09:33.527292 Epoch 4  	Train Loss = 21.55508 Val Loss = 21.00900
2024-09-13 06:11:59.317554 Epoch 5  	Train Loss = 20.35175 Val Loss = 22.23939
2024-09-13 06:14:24.824510 Epoch 6  	Train Loss = 19.96242 Val Loss = 20.08172
2024-09-13 06:16:51.139399 Epoch 7  	Train Loss = 19.08116 Val Loss = 19.97852
2024-09-13 06:19:17.098555 Epoch 8  	Train Loss = 18.79442 Val Loss = 19.15275
2024-09-13 06:21:42.110930 Epoch 9  	Train Loss = 18.44705 Val Loss = 18.93925
2024-09-13 06:24:06.491450 Epoch 10  	Train Loss = 18.21866 Val Loss = 18.57106
2024-09-13 06:26:27.367113 Epoch 11  	Train Loss = 17.96095 Val Loss = 18.72773
2024-09-13 06:28:48.064328 Epoch 12  	Train Loss = 17.80346 Val Loss = 18.64092
2024-09-13 06:31:08.554027 Epoch 13  	Train Loss = 17.67186 Val Loss = 18.66442
2024-09-13 06:33:29.056519 Epoch 14  	Train Loss = 17.49100 Val Loss = 18.33118
2024-09-13 06:35:51.589946 Epoch 15  	Train Loss = 17.39928 Val Loss = 18.68103
2024-09-13 06:38:15.261279 Epoch 16  	Train Loss = 16.70307 Val Loss = 17.71487
2024-09-13 06:40:35.715668 Epoch 17  	Train Loss = 16.55267 Val Loss = 17.70137
2024-09-13 06:42:58.089024 Epoch 18  	Train Loss = 16.50793 Val Loss = 17.68988
2024-09-13 06:45:20.266663 Epoch 19  	Train Loss = 16.46007 Val Loss = 17.72051
2024-09-13 06:47:42.341808 Epoch 20  	Train Loss = 16.43071 Val Loss = 17.68660
2024-09-13 06:50:04.219029 Epoch 21  	Train Loss = 16.39902 Val Loss = 17.72146
2024-09-13 06:52:25.915375 Epoch 22  	Train Loss = 16.37216 Val Loss = 17.70957
2024-09-13 06:54:47.968900 Epoch 23  	Train Loss = 16.34438 Val Loss = 17.64294
2024-09-13 06:57:09.626716 Epoch 24  	Train Loss = 16.31900 Val Loss = 17.68341
2024-09-13 06:59:31.604080 Epoch 25  	Train Loss = 16.29233 Val Loss = 17.64735
2024-09-13 07:01:53.189158 Epoch 26  	Train Loss = 16.27527 Val Loss = 17.67556
2024-09-13 07:04:14.679442 Epoch 27  	Train Loss = 16.24201 Val Loss = 17.64044
2024-09-13 07:06:36.705095 Epoch 28  	Train Loss = 16.22903 Val Loss = 17.69305
2024-09-13 07:08:58.918534 Epoch 29  	Train Loss = 16.21078 Val Loss = 17.64805
2024-09-13 07:11:20.083741 Epoch 30  	Train Loss = 16.17397 Val Loss = 17.67128
2024-09-13 07:13:42.132804 Epoch 31  	Train Loss = 16.09679 Val Loss = 17.59664
2024-09-13 07:16:03.960012 Epoch 32  	Train Loss = 16.07872 Val Loss = 17.61039
2024-09-13 07:18:25.739448 Epoch 33  	Train Loss = 16.06937 Val Loss = 17.61552
2024-09-13 07:20:47.480826 Epoch 34  	Train Loss = 16.07238 Val Loss = 17.61593
2024-09-13 07:23:09.047052 Epoch 35  	Train Loss = 16.06495 Val Loss = 17.60575
2024-09-13 07:25:31.408529 Epoch 36  	Train Loss = 16.06521 Val Loss = 17.61679
Early stopping at epoch: 36
Best at epoch 31:
Train Loss = 16.09679
Train RMSE = 27.59202, MAE = 16.54747, MAPE = 11.90275
Val Loss = 17.59664
Val RMSE = 30.61927, MAE = 18.24816, MAPE = 11.80398
Saved Model: ../saved_models/DSTCGCN-PEMS04-2024-09-13-05-59-49.pt
--------- Test ---------
All Steps RMSE = 30.13880, MAE = 18.20557, MAPE = 12.05503
Step 1 RMSE = 27.09042, MAE = 16.64290, MAPE = 11.10640
Step 2 RMSE = 28.05513, MAE = 17.09564, MAPE = 11.40547
Step 3 RMSE = 28.78766, MAE = 17.46827, MAPE = 11.64001
Step 4 RMSE = 29.35461, MAE = 17.76757, MAPE = 11.80554
Step 5 RMSE = 29.82236, MAE = 18.01524, MAPE = 11.93141
Step 6 RMSE = 30.21878, MAE = 18.22994, MAPE = 12.04066
Step 7 RMSE = 30.58722, MAE = 18.43180, MAPE = 12.18142
Step 8 RMSE = 30.91338, MAE = 18.61802, MAPE = 12.29799
Step 9 RMSE = 31.21088, MAE = 18.80017, MAPE = 12.39378
Step 10 RMSE = 31.47313, MAE = 18.95530, MAPE = 12.51014
Step 11 RMSE = 31.71987, MAE = 19.12286, MAPE = 12.60897
Step 12 RMSE = 32.00218, MAE = 19.31897, MAPE = 12.73844
Inference time: 15.26 s
