PEMS08
Trainset:	x-(10700, 12, 170, 3)	y-(10700, 12, 170, 1)
Valset:  	x-(3567, 12, 170, 3)  	y-(3567, 12, 170, 1)
Testset:	x-(3566, 12, 170, 3)	y-(3566, 12, 170, 1)

--------- DSTCGCN ---------
{
    "num_nodes": 170,
    "in_steps": 12,
    "out_steps": 12,
    "train_size": 0.6,
    "val_size": 0.2,
    "time_of_day": true,
    "day_of_week": true,
    "lr": 0.001,
    "weight_decay": 0.0015,
    "milestones": [
        25,
        45,
        65
    ],
    "lr_decay_rate": 0.1,
    "batch_size": 16,
    "max_epochs": 300,
    "early_stop": 30,
    "use_cl": false,
    "cl_step_size": 2500,
    "model_args": {
        "num_nodes": 170,
        "in_steps": 12,
        "out_steps": 12,
        "steps_per_day": 288,
        "input_dim": 3,
        "output_dim": 1,
        "input_embedding_dim": 24,
        "tod_embedding_dim": 24,
        "dow_embedding_dim": 24,
        "spatial_embedding_dim": 0,
        "adaptive_embedding_dim": 80,
        "feed_forward_dim": 256,
        "num_heads": 4,
        "num_layers": 3,
        "dropout": 0.1
    }
}
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
DSTCGCN                                  [16, 12, 170, 1]          165,020
├─Linear: 1-1                            [16, 12, 170, 24]         96
├─Embedding: 1-2                         [16, 12, 170, 24]         6,912
├─Embedding: 1-3                         [16, 12, 170, 24]         168
├─ModuleList: 1-4                        --                        --
│    └─SelfAttentionLayer: 2-1           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-1          [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-2                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-3               [16, 170, 12, 152]        304
│    │    └─Sequential: 3-4              [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-5                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-6               [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-2           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-7          [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-8                 [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-9               [16, 170, 12, 152]        304
│    │    └─Sequential: 3-10             [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-11                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-12              [16, 170, 12, 152]        304
│    └─SelfAttentionLayer: 2-3           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-13         [16, 170, 12, 152]        93,024
│    │    └─Dropout: 3-14                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-15              [16, 170, 12, 152]        304
│    │    └─Sequential: 3-16             [16, 170, 12, 152]        78,232
│    │    └─Dropout: 3-17                [16, 170, 12, 152]        --
│    │    └─LayerNorm: 3-18              [16, 170, 12, 152]        304
├─ModuleList: 1-5                        --                        --
│    └─SelfAttentionLayer: 2-4           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-19         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-20                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-21              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-22             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-23                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-24              [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-5           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-25         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-26                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-27              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-28             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-29                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-30              [16, 12, 170, 152]        304
│    └─SelfAttentionLayer: 2-6           [16, 12, 170, 152]        --
│    │    └─AttentionLayer: 3-31         [16, 12, 170, 152]        93,024
│    │    └─Dropout: 3-32                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-33              [16, 12, 170, 152]        304
│    │    └─Sequential: 3-34             [16, 12, 170, 152]        78,232
│    │    └─Dropout: 3-35                [16, 12, 170, 152]        --
│    │    └─LayerNorm: 3-36              [16, 12, 170, 152]        304
├─ModuleList: 1-6                        --                        --
│    └─FFTSelector: 2-7                  [12, 3]                   --
│    │    └─Linear: 3-37                 [16, 12, 256]             6,615,296
│    │    └─Linear: 3-38                 [16, 12, 256]             6,615,296
├─ModuleList: 1-7                        --                        --
│    └─CGCN: 2-8                         [16, 12, 170, 152]        344
│    │    └─LayerNorm: 3-39              [170, 10]                 20
│    │    └─Dropout: 3-40                [170, 10]                 --
│    │    └─GCN: 3-41                    [16, 510, 152]            463,904
│    │    └─LayerNorm: 3-42              [170, 10]                 (recursive)
│    │    └─Dropout: 3-43                [170, 10]                 --
│    │    └─GCN: 3-44                    [16, 510, 152]            (recursive)
│    │    └─LayerNorm: 3-45              [170, 10]                 (recursive)
│    │    └─Dropout: 3-46                [170, 10]                 --
│    │    └─GCN: 3-47                    [16, 510, 152]            (recursive)
│    │    └─LayerNorm: 3-48              [170, 10]                 (recursive)
│    │    └─Dropout: 3-49                [170, 10]                 --
│    │    └─GCN: 3-50                    [16, 510, 152]            (recursive)
│    │    └─LayerNorm: 3-51              [170, 10]                 (recursive)
│    │    └─Dropout: 3-52                [170, 10]                 --
│    │    └─GCN: 3-53                    [16, 510, 152]            (recursive)
│    │    └─LayerNorm: 3-54              [170, 10]                 (recursive)
│    │    └─Dropout: 3-55                [170, 10]                 --
│    │    └─GCN: 3-56                    [16, 510, 152]            (recursive)
│    │    └─LayerNorm: 3-57              [170, 10]                 (recursive)
│    │    └─Dropout: 3-58                [170, 10]                 --
│    │    └─GCN: 3-59                    [16, 510, 152]            (recursive)
│    │    └─LayerNorm: 3-60              [170, 10]                 (recursive)
│    │    └─Dropout: 3-61                [170, 10]                 --
│    │    └─GCN: 3-62                    [16, 510, 152]            (recursive)
│    │    └─LayerNorm: 3-63              [170, 10]                 (recursive)
│    │    └─Dropout: 3-64                [170, 10]                 --
│    │    └─GCN: 3-65                    [16, 510, 152]            (recursive)
│    │    └─LayerNorm: 3-66              [170, 10]                 (recursive)
│    │    └─Dropout: 3-67                [170, 10]                 --
│    │    └─GCN: 3-68                    [16, 510, 152]            (recursive)
│    │    └─LayerNorm: 3-69              [170, 10]                 (recursive)
│    │    └─Dropout: 3-70                [170, 10]                 --
│    │    └─GCN: 3-71                    [16, 510, 152]            (recursive)
│    │    └─LayerNorm: 3-72              [170, 10]                 (recursive)
│    │    └─Dropout: 3-73                [170, 10]                 --
│    │    └─GCN: 3-74                    [16, 510, 152]            (recursive)
│    │    └─LayerNorm: 3-75              [16, 12, 170, 152]        304
├─Linear: 1-8                            [16, 170, 12]             21,900
==========================================================================================
Total params: 14,920,444
Trainable params: 14,920,444
Non-trainable params: 0
Total mult-adds (M): 228.76
==========================================================================================
Input size (MB): 0.39
Forward/backward pass size (MB): 2246.84
Params size (MB): 57.17
Estimated Total Size (MB): 2304.40
==========================================================================================

Loss: HuberLoss

2024-09-12 03:08:27.900749 Epoch 1  	Train Loss = 28.84239 Val Loss = 25.34825
2024-09-12 03:09:45.800665 Epoch 2  	Train Loss = 20.60821 Val Loss = 21.54639
2024-09-12 03:11:03.450619 Epoch 3  	Train Loss = 19.05584 Val Loss = 19.33540
2024-09-12 03:12:22.231421 Epoch 4  	Train Loss = 17.33191 Val Loss = 16.85909
2024-09-12 03:13:40.430460 Epoch 5  	Train Loss = 16.87590 Val Loss = 16.00452
2024-09-12 03:14:58.765084 Epoch 6  	Train Loss = 16.00938 Val Loss = 15.92179
2024-09-12 03:16:17.253343 Epoch 7  	Train Loss = 15.88788 Val Loss = 16.08919
2024-09-12 03:17:35.570705 Epoch 8  	Train Loss = 15.46415 Val Loss = 15.02565
2024-09-12 03:18:54.915923 Epoch 9  	Train Loss = 15.12838 Val Loss = 14.91898
2024-09-12 03:20:14.757876 Epoch 10  	Train Loss = 14.93460 Val Loss = 14.70884
2024-09-12 03:21:34.446680 Epoch 11  	Train Loss = 14.65229 Val Loss = 14.48811
2024-09-12 03:22:53.445096 Epoch 12  	Train Loss = 14.55376 Val Loss = 14.77898
2024-09-12 03:24:12.462331 Epoch 13  	Train Loss = 14.35070 Val Loss = 14.82433
2024-09-12 03:25:31.466648 Epoch 14  	Train Loss = 14.23521 Val Loss = 15.01221
2024-09-12 03:26:50.840907 Epoch 15  	Train Loss = 14.02910 Val Loss = 14.43892
2024-09-12 03:28:10.279570 Epoch 16  	Train Loss = 14.00976 Val Loss = 13.90117
2024-09-12 03:29:29.742176 Epoch 17  	Train Loss = 13.75671 Val Loss = 13.75640
2024-09-12 03:30:48.881419 Epoch 18  	Train Loss = 13.66479 Val Loss = 13.96615
2024-09-12 03:32:07.776522 Epoch 19  	Train Loss = 13.61813 Val Loss = 13.79463
2024-09-12 03:33:26.778645 Epoch 20  	Train Loss = 13.54237 Val Loss = 13.86382
2024-09-12 03:34:45.957477 Epoch 21  	Train Loss = 13.47700 Val Loss = 14.06032
2024-09-12 03:36:05.619676 Epoch 22  	Train Loss = 13.39472 Val Loss = 13.84586
2024-09-12 03:37:25.120437 Epoch 23  	Train Loss = 13.35186 Val Loss = 13.75975
2024-09-12 03:38:44.717783 Epoch 24  	Train Loss = 13.30616 Val Loss = 13.65588
2024-09-12 03:40:04.143979 Epoch 25  	Train Loss = 13.22528 Val Loss = 13.75286
2024-09-12 03:41:23.574025 Epoch 26  	Train Loss = 12.65149 Val Loss = 13.26079
2024-09-12 03:42:42.633385 Epoch 27  	Train Loss = 12.55943 Val Loss = 13.22771
2024-09-12 03:44:01.847793 Epoch 28  	Train Loss = 12.52822 Val Loss = 13.23382
2024-09-12 03:45:22.185537 Epoch 29  	Train Loss = 12.50940 Val Loss = 13.28388
2024-09-12 03:46:41.464022 Epoch 30  	Train Loss = 12.48843 Val Loss = 13.27104
2024-09-12 03:48:00.936255 Epoch 31  	Train Loss = 12.46850 Val Loss = 13.29134
2024-09-12 03:49:20.445345 Epoch 32  	Train Loss = 12.44967 Val Loss = 13.27158
2024-09-12 03:50:39.868511 Epoch 33  	Train Loss = 12.43304 Val Loss = 13.30900
2024-09-12 03:51:59.311334 Epoch 34  	Train Loss = 12.41889 Val Loss = 13.26709
2024-09-12 03:53:19.571241 Epoch 35  	Train Loss = 12.40393 Val Loss = 13.29800
2024-09-12 03:54:38.831084 Epoch 36  	Train Loss = 12.39060 Val Loss = 13.27576
2024-09-12 03:55:57.950199 Epoch 37  	Train Loss = 12.37889 Val Loss = 13.28013
2024-09-12 03:57:17.254517 Epoch 38  	Train Loss = 12.36396 Val Loss = 13.22970
2024-09-12 03:58:36.687467 Epoch 39  	Train Loss = 12.35529 Val Loss = 13.27902
2024-09-12 03:59:55.989046 Epoch 40  	Train Loss = 12.34236 Val Loss = 13.30694
2024-09-12 04:01:15.417608 Epoch 41  	Train Loss = 12.33224 Val Loss = 13.28889
2024-09-12 04:02:34.917907 Epoch 42  	Train Loss = 12.31900 Val Loss = 13.30413
2024-09-12 04:03:53.984703 Epoch 43  	Train Loss = 12.31220 Val Loss = 13.38326
2024-09-12 04:05:13.032034 Epoch 44  	Train Loss = 12.29783 Val Loss = 13.26954
2024-09-12 04:06:32.250350 Epoch 45  	Train Loss = 12.28822 Val Loss = 13.30434
2024-09-12 04:07:51.270085 Epoch 46  	Train Loss = 12.22366 Val Loss = 13.26325
2024-09-12 04:09:10.356498 Epoch 47  	Train Loss = 12.21539 Val Loss = 13.26134
2024-09-12 04:10:29.992856 Epoch 48  	Train Loss = 12.21102 Val Loss = 13.25960
2024-09-12 04:11:51.419870 Epoch 49  	Train Loss = 12.20975 Val Loss = 13.25534
2024-09-12 04:13:10.701971 Epoch 50  	Train Loss = 12.20674 Val Loss = 13.26128
2024-09-12 04:14:30.000246 Epoch 51  	Train Loss = 12.20457 Val Loss = 13.25987
2024-09-12 04:15:49.471183 Epoch 52  	Train Loss = 12.20309 Val Loss = 13.26289
2024-09-12 04:17:08.469709 Epoch 53  	Train Loss = 12.20096 Val Loss = 13.27393
2024-09-12 04:18:27.496766 Epoch 54  	Train Loss = 12.20143 Val Loss = 13.25763
2024-09-12 04:19:49.227126 Epoch 55  	Train Loss = 12.19801 Val Loss = 13.27297
2024-09-12 04:21:08.721463 Epoch 56  	Train Loss = 12.19846 Val Loss = 13.27447
2024-09-12 04:22:28.038578 Epoch 57  	Train Loss = 12.19529 Val Loss = 13.26281
Early stopping at epoch: 57
Best at epoch 27:
Train Loss = 12.55943
Train RMSE = 22.48747, MAE = 12.79688, MAPE = 8.53192
Val Loss = 13.22771
Val RMSE = 24.03224, MAE = 13.66305, MAPE = 10.39642
Saved Model: ../saved_models/DSTCGCN-PEMS08-2024-09-12-03-07-04.pt
--------- Test ---------
All Steps RMSE = 23.19046, MAE = 13.52467, MAPE = 8.96332
Step 1 RMSE = 19.55835, MAE = 11.83487, MAPE = 7.83632
Step 2 RMSE = 20.62335, MAE = 12.25036, MAPE = 8.12228
Step 3 RMSE = 21.46595, MAE = 12.62105, MAPE = 8.33941
Step 4 RMSE = 22.14683, MAE = 12.92752, MAPE = 8.54227
Step 5 RMSE = 22.71958, MAE = 13.20413, MAPE = 8.73278
Step 6 RMSE = 23.20745, MAE = 13.45081, MAPE = 8.93496
Step 7 RMSE = 23.65428, MAE = 13.69607, MAPE = 9.06991
Step 8 RMSE = 24.05734, MAE = 13.92499, MAPE = 9.20662
Step 9 RMSE = 24.42305, MAE = 14.13058, MAPE = 9.36180
Step 10 RMSE = 24.76750, MAE = 14.34137, MAPE = 9.51066
Step 11 RMSE = 25.14106, MAE = 14.58406, MAPE = 9.73312
Step 12 RMSE = 25.67914, MAE = 15.33026, MAPE = 10.16979
Inference time: 8.54 s
